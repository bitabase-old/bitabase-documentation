<html>
  <head>
    <title>Blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="http://127.0.0.1:9000/css/header.css" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/tomorrow.min.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js"></script>
    <script charset="UTF-8" src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/javascript.min.js"></script>
    <script>
      document.addEventListener('DOMContentLoaded', (event) => {
        document.querySelectorAll('pre code').forEach((block) => {
          hljs.highlightBlock(block);
        });
      });
    </script>
    <base href="/" />

    <link rel="icon" type="image/png" href="/img/favicon.png" />
  </head>

  <body>
    <header>
      <div class="logo">
        <a href="/">
          <img src="/img/favicon.png" class="logo-icon">
          <img src="/img/logo-white.png" class="logo-name">
        </a>
      </div>
      
    <nav>
      <a href="http://127.0.0.1:9000/">Home</a>
      <a href="http://127.0.0.1:9000/tutorial" class="false">Tutorial</a>
      <a href="http://127.0.0.1:9000/databases">Your Databases</a>
      <a href="http://127.0.0.1:9000/pricing">Pricing</a>
      <a href="http://127.0.0.1:9000/community">Community</a>
      <a href="/" false>Docs</a>
      <a href="/blog" class="active">Blog</a>
      <a href="http://127.0.0.1:9000/support">Support</a>
    </nav>
  
    </header>

    <main>
      <div class="with-sidebar"><div class="thin content"><sidebar><ul><li><span>August 2020</span><ul><li><span>3</span><a href="/blog/2020/07/03/sub-queries">Sub queries</a></li><li><span>1</span><a href="/blog/2020/07/01/paging">Paging</a></li></ul></li><li><span>July 2020</span><ul><li><span>7</span><a href="/blog/2020/06/07/removing-users">Removing users</a></li><li><span>6</span><a href="/blog/2020/06/06/benchmarking">Benchmarking</a></li></ul></li><li><span>June 2020</span><ul><li><span>29</span><a href="/blog/2020/05/29/stress-testing-results">Stress testing results</a></li><li><span>25</span><a href="/blog/2020/05/25/running-tests">Running tests</a></li><li><span>21</span><a href="/blog/2020/05/21/service-discovery-included">Service discovery included</a></li></ul></li></ul></sidebar><section><h1>Introducing sub queries<small>Published by <a href="http://twitter.com/markiswylde" target="_blank">Mark Wylde</a> on 2020-08-03</small></h1><div class="content-info"><a target="_blank" class="edit-page" href="https://github.com/bitabase/bitabase-documentation/blob/master/content/blog/2020-08-03-sub-queries.md">Edit this page</a><em>Last updated: 2020-08-10</em></div><div><p>Some updates for the work done this month.</p>
<h2 id="performance">Performance</h2>
<p>I managed to spin up some VM&#39;s on a cloud host provider and ran bitabase-stressed getting around 600 writes per second, which was a shame but not unexpected.</p>
<p>What was surprising is rqlite was no longer taking the majority of CPU. The node process was capping out at around 70%, which still left me with some unanswered questions.</p>
<p>So I decided to create a <a href="https://github.com/bitabase/barely-benchmarked">benchmarking app</a> which consists of two simple servers. One that talks to bitabase-server (not the clustered bitabase, only the server), and another that talks to postgres.</p>
<p>They both ended up maxing out at 600 requests per second, which makes me think there something about NodeJS&#39;s networking that I don&#39;t understand. My next step is to play around with a server that doesn&#39;t connect to either postgres or bitabase and see how many requests I can get.</p>
<h2 id="users">Users</h2>
<p>In a previous release of bitabase-server, I removed the users&#39; functionality, but there was still some remaining. Namely, every request was trying to parse users if credentials have been provided.</p>
<p>With the latest server release, this has been completely removed, along with the remaining tests. Authentication is no longer available in bitabase. Unless you implement it yourself. More on that next...</p>
<h2 id="sub-queries">Sub queries</h2>
<p>I have added a first attempt at sub queries into the bitabase server, and a small change to the gateway to allow querying databases when the host domain does not match up.</p>
<p>Having a look at the test in bitabase-server we can see the following collection config:</p>
<pre><code class="language-javascript">// Create a new collection called &#39;groups&#39;
yield righto(callarestJson, {
  url: &#39;http://localhost:8000/v1/databases/test/collections&#39;,
  method: &#39;post&#39;,
  body: {
    name: &#39;groups&#39;
  }
});

// Create a new record in &#39;groups&#39; with key &#39;admin&#39;
yield righto(callarestJson, {
  url: &#39;http://localhost:8000/v1/databases/test/records/categories&#39;,
  method: &#39;post&#39;,
  body: {
    key: &#39;admin&#39;, value: &#39;Administrator&#39;
  }
});

// Create a new record in &#39;groups&#39; with key &#39;b&#39;
yield righto(callarestJson, {
  url: &#39;http://localhost:8000/v1/databases/test/records/categories&#39;,
  method: &#39;post&#39;,
  body: {
    key: &#39;user&#39;, value: &#39;Standard User&#39;
  }
});

// Create a new test collection that will add a `lookupValue` key
yield righto(callarestJson, {
  url: &#39;http://localhost:8000/v1/databases/test/collections&#39;,
  method: &#39;post&#39;,
  body: {
    name: &#39;users&#39;,
    transducers: [`
      {
        ...body,
        lookupValue: bitabase.getOne(&#39;categories&#39;, {
          query: {
            key: &#39;user&#39;
          }
        }).value}
    `]
  }
});

// Create a new record in the test collection
const testInsert = yield righto(callarestJson, {
  url: &#39;http://localhost:8000/v1/databases/test/records/users&#39;,
  method: &#39;post&#39;,
  body: {
    name: &#39;Joe Bloggs&#39;
  }
});</code></pre>
<p>This shows us the addition of a new command in the reducer evaluation scope called <code>bitabase.getOne</code>.</p>
<p>This works by going back to the gateway and performing a query. You can only query collections inside the same database and the presenters and transducers will run as if it was hit by an external request.</p>
<p>If you want to bypass some transducers, you will need to implement logic in your code. Maybe adding an authorized header that you could check may come in helpful, but I won&#39;t implement it until we have a use case.</p>
<h2 id="branding">Branding</h2>
<p>The logo and icons where inconsistent across the github profile, documentation and repos. I&#39;ve updated them all to make them more consistent. I used Affinity Designer for this, and stored the afdesign file in the bitabase-documentation repo.</p>
</div><hr><h1>Paging and Optimization<small>Published by <a href="http://twitter.com/markiswylde" target="_blank">Mark Wylde</a> on 2020-08-01</small></h1><div class="content-info"><a target="_blank" class="edit-page" href="https://github.com/bitabase/bitabase-documentation/blob/master/content/blog/2020-08-01-paging.md">Edit this page</a><em>Last updated: 2020-08-10</em></div><div><p>Some updates for the work done this month.</p>
<h2 id="performance">Performance</h2>
<p>I&#39;m still not completely sure why we&#39;re getting such a low write per second with the bitabase stack. When using the bitabase-server directly, we&#39;re getting almost 5000 write requests a second, which is more than what I wanted. But when put in the full clustered stack, we get loads of timeouts, slow reponses and high memory/cpu usage.</p>
<p>When using the bitabase-stressed project, I did see that the rqlite server seems to be the cause. So I thought about caching the results, anytime we hit it. But the gateway only hits rqlite when a server doesn&#39;t know about the collection.</p>
<p>The last month I&#39;ve pushed this down on my priorities. I originally tried testing directly on my mac, which was causing a lot of problems, then I tried inside a VM which seems to be better. I think a better step forward, would be to create a VM on an actual host, and try some stress testing there. I&#39;ll also aim to test postgres too, to ensure there&#39;s a fair comparison.</p>
<h2 id="pagination">Pagination</h2>
<p>In the meantime, I&#39;ve been struggling with pagination. I&#39;m not sure why I never thought of this before, but we need to have the ability to order records in a collection and paginate the results.</p>
<p>Let&#39;s say, for example, we want 10 records per page, and we want page 1. Under the current implementation, the gateway goes out to every server, asking them for 10 records, then zipping them together, chopping off the excess records. Stage two of this implementation was to resort them at the gateway, in a local sqlite database, then send them out.</p>
<p>But I had failed to realize this doesn&#39;t really work for getting a page higher than 1. If we want to bring back page 2, we need to know what was brought back on page 1. Again not really a problem, but when we get to page 100, this means the gateway needsto fetch 1000 records, maybe more, which will end up with performance issues.</p>
<p>So I&#39;ve thought about a slightly different approach. When searching for records, the gateway will go out to each server, and forward the query. However, this query will only return the id and fields that where sorted, and will return a maximum of the page * itemsPerPage. As these partial records are being streamed into the gateway, we resort the records and send.</p>
<p>We won&#39;t be grabbing the full record of unnecessary pages. Just the id and sort fields. Then once we have our list of records, we can go out to the servers again and filter the collection by id.</p>
<p>I think the streaming approach could offer several benefits too. There might be something around not having to hit each server unnecessarily. At first, we need to go out to every server to find the first record they hold. However, we only need to hit a server if we need a record higher than the servers first result. I don&#39;t think something like this would be implemented for a while, as we would need to implement some sort of pull stream logic between the servers and gateways.</p>
<h2 id="fields">Fields</h2>
<p>To get started with implementing the new pagination feature, I have added a <code>fields</code> querystring to the bitabase-server.</p>
<p>You can now select the fields you want to bring back via a request like:</p>
<pre><code>/v1/databases/:databaseName/records/:collectionName?fields=[&quot;id&quot;,&quot;dateCreated&quot;]&amp;order=desc(dateCreated)&amp;limit=10</code></pre>
<p>This should mean the gateway can pull back a very small amount of data per request, then after bring back the full records via:</p>
<pre><code>/v1/databases/:databaseName/records/:collectionName?query={&quot;id&quot;:{&quot;$in&quot;:[&quot;aaaa&quot;,&quot;bbbb&quot;,&quot;cccc&quot;]}}</code></pre>
<h2 id="release">Release</h2>
<p>As for now, I updated all the projects a few days ago, and released a new minor version of the main bitabase project. This simply updates all the dependencies across the projects, and fixed the path to pack sqlite3.node into the binary as it changed with a new release of sqlite of node.</p>
<p>The <code>fields</code> feature is in the bitabase-server master branch, but not in the bitabase binary. I&#39;ll release this sometime this month.</p>
<h2 id="whats-next">What&#39;s next?</h2>
<p>I really want to get back to the performance issues. When thinking about this project a year ago, I really though the bottleneck would be the bitabase-server, writing many records to a sqlite database. I&#39;m really not concerned anymore this will be an issue. The performance problems seems to be between the gateway, rqlite and the servers. Under the assumption that NodeJS can handle around 10k requests per second, and in the stress testing the network traffic is pretty low, I&#39;m guessing I&#39;m doing something inefficient that I&#39;ve forgotten about. Either that, or my Mac setup is far from production reality.</p>
<p>Next I will spin up a VM on a cloud host, and bring up a 1 server, 1 manager, 1 gateway, 1 rqlite instance on a small machine. If I&#39;m lucky, it&#39;ll be better than or at least on par with postgres, and if not, at least I&#39;ll be able to continue testing, knowing my stress tester is not part of the problem, as I&#39;ll run the tester on a separate VM.</p>
<p>As for the pagination, I&#39;m happy I have a potential solution, so I&#39;m planning on leaving it for a while. It shouldn&#39;t take too much work to make the gateway hit the servers first to return X small records, order them then query the full records. I&#39;ll save that for when I&#39;m bored with the testing.</p>
</div><hr><h1>Removing Users<small>Published by <a href="http://twitter.com/markiswylde" target="_blank">Mark Wylde</a> on 2020-07-07</small></h1><div class="content-info"><a target="_blank" class="edit-page" href="https://github.com/bitabase/bitabase-documentation/blob/master/content/blog/2020-07-07-removing-users.md">Edit this page</a><em>Last updated: 2020-08-10</em></div><div><p>As I mentioned in the last blog post, there is an undocumented feature which allows a special
collection <code>users</code> to be &quot;logged into&quot; and therefore restrict certain actions on another collection.</p>
<p>The idea was that if you have headers <code>username</code> and <code>password</code>, the server would try and find a record
in a <code>users</code> collection, and pass that into the scope of the initial reducer.</p>
<p>This was really a workaround of the async problem in reducers, before we moved the main language to
<a href="https://github.com/korynunn/presh">presh</a> where async is made very simple.</p>
<p>Once the benchmarking and stress testing has been complete and code improvements have put the project
to a satisfactory requests per second, I&#39;ll be implementing internal operations.</p>
<h2 id="internal-operations">Internal Operations</h2>
<p>The specifications may change as I&#39;m implementing it, but my thoughts can be seen in some examples below.</p>
<p>For all these examples, pretend there is a <code>users</code> table with a <code>username</code>, <code>password</code> and of course <code>id</code> field.</p>
<h3 id="simple">Simple</h3>
<p>In the example below the first reducer searches in the <code>users</code> table for a record where the <code>username</code> and <code>password</code>
match their corresponding values in the request header.</p>
<pre><code class="language-javascript">const collection = {
  name: &#39;messages&#39;,
  reducers: [
    `{
      ...body,
      user: get(&#39;users&#39;, {
        username: headers[&#39;X-Username&#39;],
        password: headers[&#39;X-Password&#39;],
      })
    }`,

    `{ ...body, userId: user ? user.id : reject(&#39;401&#39;, &#39;You are not logged in&#39;) }`
  ]
}</code></pre>
<p>Therefore to post a new <code>message</code> you could do:</p>
<pre><code class="language-javascript">fetch(&#39;https://test.bitabase.com/messages&#39;, {
  method: &#39;post&#39;,
  headers: {
    &#39;X-Username&#39;: &#39;mark&#39;,
    &#39;X-Password&#39;: &#39;supersecret&#39;
  }
})</code></pre>
<p>So long as a record if users exists in the <code>users</code> collection for <code>mark</code> and the plain text password
is <code>supersecret</code> then the record will be posted and the userId field will be set correctly.</p>
<p>If the user can not be found, it will reject to the client with a <code>401</code>.</p>
<h3 id="hashing">Hashing</h3>
<p>In the above example we stored the users password in plaintext. This is obviously terrible, so lets
hash the password. If we look at the <a href="https://docs.bitabase.com/docs/api/scripting">scripting</a>
documentation, we can see two methods <code>hashText</code> and <code>verifyHash</code>.</p>
<p>So let&#39;s ensure when we create a user their password is hashed.</p>
<pre><code class="language-javascript">const collection = {
  name: &#39;users&#39;,
  reducers: [
    `{ ...body, password: hashText(body.password) }`
  ]
}</code></pre>
<p>The above schema ensures anytime a user is <code>post</code>, <code>put</code>, <code>patched</code>, there password will be hashed and
the plain text never stored.</p>
<p>But now we need to authenticate the user. Let&#39;s take the same code from the &quot;Basic&quot; example above,
verifying the password&#39;s hash.</p>
<pre><code class="language-javascript">const collection = {
  name: &#39;messages&#39;,
  reducers: [
    `{
      ...body,
      user: get(&#39;users&#39;, {
        username: headers[&#39;X-Username&#39;]
      })
    }`,

    `{
      ...body,
      user: verifyHash(headers[&#39;X-Password&#39;], user.password) ? user : null
    }`,

    `{ ...body, userId: user ? user.id : reject(&#39;401&#39;, &#39;You are not logged in&#39;) }`
  ]
}</code></pre>
<p>Now when we post to <code>messages</code>, the above reducers will:
Reducer 1: Get a users record from the <code>users</code> table, matching only on <code>X-Username</code>.
Reducer 2: Compare the hashed password with the plaintext one, setting the user to null if it does not match.
Reducer 3: Reject if the user does not exist</p>
<h3 id="sessions">Sessions</h3>
<p>It&#39;s probably not great practice to continuously send the username and password with every requests, so why
don&#39;t we introduce the concept of sessions into our database.</p>
<pre><code class="language-javascript">const usersCollection = {
  name: &#39;users&#39;,
  reducers: [
    // Hash the users password when they create or update their account
    `{ ...body, password: hashText(body.password) }`
  ]
}

const sessionsCollection = {
  name: &#39;sessions&#39;,
  reducers: [
    // Allow anyone to create a session, but only internal requests can get and list
    `request.isInternal || method === &#39;post&#39; ? { ...body, ...headers } : reject(&#39;403&#39;, &#39;Forbidden&#39;)`,

    // Find the user in the post body when creating a new session
    `{ user: get(&#39;users&#39;, { username: body.username }) }`,

    // Ensure their password matched the stored hashed one
    `{ user: verifyHash(headers[&#39;X-Password&#39;], user.password) ? user : null }`,

    // Generate a secret random string for the token or reject if not logged in
    `user ? { token: generateSecureRandomString(16), userId: user.id } : reject(&#39;401&#39;, &#39;You are not logged in&#39;) }`
  ]
}</code></pre>
<pre><code class="language-javascript">const collection = {
  name: &#39;messages&#39;,
  reducers: [
    // Find a session matching the request header
    `{
      ...body,
      session: get(&#39;sessions&#39;, {
        token: headers[&#39;X-Session-Token&#39;]
      }),
    }`,

    // Find the user for the previously fetched session
    `{
      ...body,
      user: get(&#39;users&#39;, {
        id: session.userId
      }),
    }`,

    // Add the userId to the record, or reject if they are not signed in
    `{ ...body, userId: user ? user.id : reject(&#39;401&#39;, &#39;You are not logged in&#39;) }`
  ]
}</code></pre>
<h3 id="other-operations">Other operations</h3>
<p>Methods for all operations would be included:</p>
<pre><code>get(collectionName, query)
post(collectionName, recordData)
put(collectionName, query, recordData)
patch(collectionName, query, partialRecordData)
delete(collectionName, query)</code></pre>
<p>Also the method <code>generateSecureRandomString</code> does not exist in scripting just yet, so I&#39;ll need to
get that implemented.</p>
<p>For now you will only be able to query collections within the same database, as an <code>isInternal</code> property
will be given, allowing you to have specific <code>reducer</code> logic for internal requests.</p>
<h2 id="conclusion">Conclusion</h2>
<p>There may be things missing from above, and that will become apparent when during implementation. But
I&#39;m confident this approach will allow complete flexibility to implement custom authentication logic.</p>
<p>I will now be removing the current hardcoded &quot;magical&quot; <code>users</code> collection from the bitabase server and
that will hopefully improve the benchmarking speeds from the previous blog post.</p>
<p>Since I&#39;m not sure how much longer the optimisation piece will take, the features above will probably
not be worked on for a while.</p>
</div><hr><h1>Benchmarking<small>Published by <a href="http://twitter.com/markiswylde" target="_blank">Mark Wylde</a> on 2020-07-06</small></h1><div class="content-info"><a target="_blank" class="edit-page" href="https://github.com/bitabase/bitabase-documentation/blob/master/content/blog/2020-07-06-benchmarking.md">Edit this page</a><em>Last updated: 2020-08-10</em></div><div><p>Since the let down of last week I&#39;ve been trying to find out why things are going so slow.</p>
<p>It turns out that running lots of connections to node on my macOS laptop is just not really
possible. It ends up crashing and hanging after only a few hundred a second.</p>
<p>I moved onto a Linux VM, and was able to get a much higher throughput.</p>
<p>After playing around with Bitabase I found that the database does tend to lock when stress
testing, but in general it doesn&#39;t seem to take up too much CPU usage. So to begin with I&#39;ve
removed the SQLite parts from the bitabase-server on my dev environment to see exactly how
many requests NodeJS can actually handle.</p>
<p>Using apachebench, I ran the following command:</p>
<pre><code class="language-bash">ab -n 10000 -c 50 -k http://127.0.0.1:8100/</code></pre>
<p>This tests against a simple NodeJS server:</p>
<pre><code class="language-javascript">const http = require(&#39;http&#39;);

let requestNumber = 0;
function handler (request, response) {
  requestNumber = requestNumber + 1;
  if (Number.isInteger(requestNumber / 1000)) {
    console.log(&#39;INFO: Just passed&#39;, requestNumber, &#39;requests&#39;);
  }
  response.writeHead(200, { &#39;Content-Type&#39;: &#39;text/plain&#39; });
  response.end(&#39;hello&#39;);
}

http.createServer(handler).listen(8100);</code></pre>
<p>The server does nothing but respond with the text <code>hello</code>.</p>
<p>On my Debian VM I was able to complete 10000 requests in just over 3 seconds.</p>
<p>Not as high as I would have hoped for, but in fairness this is a VM running on my mac, so
not really sure what to expect there. However, I&#39;m hoping it sets the foundation benchmark
for adding more complexity to the stress server.</p>
<p>The second thing I tried to simple connecting to Postgres and inserting a record. To no surprise
this is great. Only a couple of extra seconds to perform 10000 inserts and it worked flawlessly:</p>
<p>The code was very simplier to the above, only using the <code>pg</code> library, it inserts a record and waits
before responding to the client.</p>
<pre><code class="language-javascript">const http = require(&#39;http&#39;);
const { Pool } = require(&#39;pg&#39;)
const pool = new Pool()

let requestNumber = 0;
function handler (request, response) {
  requestNumber = requestNumber + 1;
  if (Number.isInteger(requestNumber / 1000)) {
    console.log(&#39;INFO: Just passed&#39;, requestNumber, &#39;requests&#39;);
  }

  pool.query(`INSERT INTO test (name) VALUES (&#39;Hello&#39;)`, (err, res) =&gt; {
    if (err) {
      return console.log(err);
    }
    response.writeHead(200, { &#39;Content-Type&#39;: &#39;text/plain&#39; });
    response.end(&#39;hello&#39;);
  })
}

http.createServer(handler).listen(8100);</code></pre>
<p>Starting to think about what Bitabase is doing, I thought maybe rqlite is really
slow, and I&#39;d like to test it&#39;s performance.</p>
<p>So to start with I ran a query to select one record from the test table. This was
able to process 10000 requests in just under 10 seconds. Very disappointing, considering
I choose to disable the consistency (<code>?level=none</code>) when querying rqlite.</p>
<p>Adding to the complexiy I tried inserting records. That increased the time to just over 11 seconds,
then reading with consistency was almost 12 seconds for 10000 requests.</p>
<p>Finally I tested Bitabase, giving me 10000 requests in 18055ms. This is over 3 times slower than postgres,
and not anywhere near my expectations of Bitabase&#39;s potential.</p>
<p>I will continue to look deeper into this, and there are two main areas I want to investigate:</p>
<ol>
<li>Every transaction requires a query on the gateway to grab the database and collection schema
information, but is this actually nessisary? If it is, we could cache it locally or query the rqlite
sqlite database directly. But that&#39;s what I thought <code>?level=none</code> was going, so again quite disappointing
if this is the cause of the slowness.</li>
<li>The server looks for authentication data for every request. The current <code>user</code> functionalty isn&#39;t even
documented, but the early idea was to allow collections to authenticate automatically if a <code>users</code> table
exists. With the concept of <code>reducers</code> now well implemented, and the ability to CRUD records inside reducers
coming very soon, I think it&#39;s safe to completly remove this functionality for now.</li>
</ol>
<p>Anyway, there is still a lot of investigating and trials left to go through to get this to an acceptable
level of throughput. I&#39;m pleased that the issues I was having last weekend where to do with my mac, and
that running on a VM at least stopped the crashing and gave me a clear number to work towards.</p>
<p>You can find the results of the test below:</p>
<pre><code class="language-text">nodejs plain
   10000 requests: 3196ms
postgres
   10000 requests: 5319ms
rqlite read (no consistency)
   10000 requests: 9610ms
rqlite insert
   10000 requests: 11336ms
rqlite read
   10000 requests: 11841ms
bitabase (no save)
   10000 requests: 18055ms</code></pre>
<p>I&#39;m still hopefull I can get Bitabase transactions down to 10000 requests in around 5000ms. If I can get
anywhere near there, I&#39;ll be happy Bitabase still has potential.</p>
<p>Until now, there are no code updates to Bitabase, although I&#39;ve changed <code>bitabase-stressed</code> to ensure it
only pushed new jobs to the maximum allowed amount. Rather than accumulating a huge amount of requests when
receiving push back.</p>
</div><hr><h1>Stress Testing Results<small>Published by <a href="http://twitter.com/markiswylde" target="_blank">Mark Wylde</a> on 2020-06-29</small></h1><div class="content-info"><a target="_blank" class="edit-page" href="https://github.com/bitabase/bitabase-documentation/blob/master/content/blog/2020-06-29-stress-testing-results.md">Edit this page</a><em>Last updated: 2020-08-10</em></div><div><p>Over this weekend I&#39;ve been playing with a new project called <a href="https://github.com/bitabase/bitabase-stressed">bitabase-stressed</a>.</p>
<p><img src="/img/blog-assets/stressed.png" alt="stressed screenshot" title="stressed screenshot"></p>
<p>This is a tool that will run a series of integration tests at a rate you specify to see how far you can push the database.</p>
<p>Unfortunatly the results this weekend have not been impressive, although I haven&#39;t looked into the reason why this is slow yet.</p>
<p>I expect the rqlite stuff to be a bit slow, since it&#39;s a replicated database that is strongly consistent for metadata. This means there is always going to be a somewhat upper limit of how many databases and collections you can create. I was able to create around 250 databases a second on my machine, which I&#39;m happyish about. In the future I would want to test this can vertically scale by increasing the machine size.</p>
<p>However I would expect that writing records should be a lot better than 100 per second. This is unacceptably slow.</p>
<p>Again, I haven&#39;t been able to research why it&#39;s going so slow, but some thoughts:</p>
<ol>
<li>SQLite is normally not very good (although better than this) at inserting records one at a time. An option may be to batch all writes received within a small timeframe (100ms) then insert them at the same time.</li>
<li>This is running in docker on the same machine I&#39;m running the stress tests.</li>
<li>I haven&#39;t scaled horiontally. This was only on a &quot;cluster&quot; with one server node.</li>
<li>This sucks. I was really hoping for a few thousand writes per second per node.</li>
</ol>
<p>We&#39;re down but not out. Now we have the stress testing tool in place, I can leave that running and tweak settings and debug the services to try and find out what the bottleneck is.</p>
<p>On top of the stress testing, some bugs have been fixed. The gateway and manager were crashing when no servers had been discovered yet. Now it returns a 500 to the client, and logs an error to the console.</p>
<p>That&#39;s it for now!</p>
</div><hr></section></div></div>
    </main>
  </body>
</html>
