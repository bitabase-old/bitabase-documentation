---
title: Paging and Optimization
author: Mark Wylde
authorURL: http://twitter.com/markiswylde
---

## Performance
I'm still not completely sure why we're getting such a low write per second with the bitabase stack. When using the bitabase-server directly, we're getting almost 5000 write requests a second, which is more than what I wanted. But when put in the full clustered stack, we get loads of timeouts, slow reponses and high memory/cpu usage.

When using the bitabase-stressed project, I did see that the rqlite server seems to be the cause. So I thought about caching the results, anytime we hit it. But the gateway only hits rqlite when a server doesn't know about the collection.

The last month I've pushed this down on my priorities. I originally tried testing directly on my mac, which was causing a lot of problems, then I tried inside a VM which seems to be better. I think a better step forward, would be to create a VM on an actual host, and try some stress testing there. I'll also aim to test postgres too, to ensure there's a fair comparison.

## Pagination
In the meantime, I've been struggling with pagination. I'm not sure why I never thought of this before, but we need to have the ability to order records in a collection and paginate the results.

Let's say, for example, we want 10 records per page, and we want page 1. Under the current implementation, the gateway goes out to every server, asking them for 10 records, then zipping them together, chopping off the excess records. Stage two of this implementation was to resort them at the gateway, in a local sqlite database, then send them out.

But I had failed to realize this doesn't really work for getting a page higher than 1. If we want to bring back page 2, we need to know what was brought back on page 1. Again not really a problem, but when we get to page 100, this means the gateway needsto fetch 1000 records, maybe more, which will end up with performance issues.

So I've thought about a slightly different approach. When searching for records, the gateway will go out to each server, and forward the query. However, this query will only return the id and fields that where sorted, and will return a maximum of the page * itemsPerPage. As these partial records are being streamed into the gateway, we resort the records and send.

We won't be grabbing the full record of unnecessary pages. Just the id and sort fields. Then once we have our list of records, we can go out to the servers again and filter the collection by id.

I think the streaming approach could offer several benefits too. There might be something around not having to hit each server unnecessarily. At first, we need to go out to every server to find the first record they hold. However, we only need to hit a server if we need a record higher than the servers first result. I don't think something like this would be implemented for a while, as we would need to implement some sort of pull stream logic between the servers and gateways.

## Fields
To get started with implementing the new pagination feature, I have added a `fields` querystring to the bitabase-server.

You can now select the fields you want to bring back via a request like:

```
/v1/databases/:databaseName/records/:collectionName?fields=["id","dateCreated"]&order=desc(dateCreated)&limit=10
```

This should mean the gateway can pull back a very small amount of data per request, then after bring back the full records via:

```
/v1/databases/:databaseName/records/:collectionName?query={"id":{"$in":["aaaa","bbbb","cccc"]}}
```

## Release
As for now, I updated all the projects a few days ago, and released a new minor version of the main bitabase project. This simply updates all the dependencies across the projects, and fixed the path to pack sqlite3.node into the binary as it changed with a new release of sqlite of node.

The `fields` feature is in the bitabase-server master branch, but not in the bitabase binary. I'll release this sometime this month.

## What's next?
I really want to get back to the performance issues. When thinking about this project a year ago, I really though the bottleneck would be the bitabase-server, writing many records to a sqlite database. I'm really not concerned anymore this will be an issue. The performance problems seems to be between the gateway, rqlite and the servers. Under the assumption that NodeJS can handle around 10k requests per second, and in the stress testing the network traffic is pretty low, I'm guessing I'm doing something inefficient that I've forgotten about. Either that, or my Mac setup is far from production reality.

Next I will spin up a VM on a cloud host, and bring up a 1 server, 1 manager, 1 gateway, 1 rqlite instance on a small machine. If I'm lucky, it'll be better than or at least on par with postgres, and if not, at least I'll be able to continue testing, knowing my stress tester is not part of the problem, as I'll run the tester on a separate VM.

As for the pagination, I'm happy I have a potential solution, so I'm planning on leaving it for a while. It shouldn't take too much work to make the gateway hit the servers first to return X small records, order them then query the full records. I'll save that for when I'm bored with the testing.
